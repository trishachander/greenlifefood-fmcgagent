{
    "model": "llama-3.1-8b-instant",
    "temperature": 0.7,
    "max_tokens": 500,
    "stop_sequences": ["\n\nUser:", "\n\nAssistant:"],
    "presence_penalty": 0.6,
    "frequency_penalty": 0.3
}